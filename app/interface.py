import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import streamlit as st
import re
import pandas as pd
import altair as alt
import fitz  # PyMuPDF

from scripts.search import SemanticSearcher
from scripts.build_index import build_faiss_index

# üì¶ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
AVAILABLE_MODELS = {
    "MiniLM (–±—ã—Å—Ç—Ä–æ)": "sentence-transformers/all-MiniLM-L6-v2",
    "MPNet (—Ç–æ—á–Ω–æ)": "sentence-transformers/all-mpnet-base-v2",
    "Multilingual": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
}
DATA_DIR = "data"

# ‚ú® –ü–æ–¥—Å–≤–µ—Ç–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
def highlight(text: str, query: str):
    keywords = re.findall(r'\w+', query.lower())
    for kw in keywords:
        text = re.sub(f"(?i)({kw})", r"**\1**", text)
    return text

# üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF
def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with fitz.open(pdf_path) as doc:
        for page in doc:
            text += page.get_text()
    return text

# üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞
def check_text_quality(text: str) -> dict:
    words = re.findall(r'\w+', text)
    return {
        "length": len(text),
        "lines": text.count("\n"),
        "unique_words": len(set(words)),
        "avg_line_length": round(len(text) / (text.count("\n") + 1), 2),
        "non_ascii": any(ord(c) > 127 for c in text),
        "symbols": any(c in "!@#$%^&*()[]{}<>?" for c in text)
    }

# üéØ –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
def main():
    st.set_page_config(page_title="Semantic Search", layout="wide")
    st.title("üîç –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –Ω–∞—É—á–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º")

    # üìå –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
    model_label = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:", list(AVAILABLE_MODELS.keys()))
    model_name = AVAILABLE_MODELS[model_label]

    # üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    st.subheader("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (.txt –∏–ª–∏ .pdf)")
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª", type=["txt", "pdf"])
    if uploaded_file:
        file_path = os.path.join(DATA_DIR, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.success(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {uploaded_file.name}")

        if uploaded_file.name.endswith(".pdf"):
            text = extract_text_from_pdf(file_path)
            txt_path = file_path.replace(".pdf", ".txt")
            with open(txt_path, "w", encoding="utf-8") as f:
                f.write(text)
            st.info(f"üìÑ PDF –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤: {os.path.basename(txt_path)}")
        else:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()

        quality = check_text_quality(text)
        st.subheader("üìä –ö–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞")
        st.json(quality)

        if st.button("üîÑ –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å"):
            with st.spinner("–û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å..."):
                build_faiss_index(model_name=model_name, data_dir="data", index_type="ip")
            st.success("‚úÖ –ò–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª—ë–Ω!")

    # üïò –ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
    if "history" not in st.session_state:
        st.session_state.history = []

    with st.sidebar:
        st.subheader("üïò –ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤")
        for q in reversed(st.session_state.history[-10:]):
            if st.button(q):
                st.session_state.query = q

    # üîé –í–≤–æ–¥ –∑–∞–ø—Ä–æ—Å–∞
    query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:", value=st.session_state.get("query", ""))
    top_k = st.slider("–°–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∫–∞–∑–∞—Ç—å:", min_value=1, max_value=10, value=3)
    score_threshold = st.slider("–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ score:", min_value=0.0, max_value=1.0, value=0.0, step=0.01)
    
    if query:
        st.session_state.history.append(query)

        with st.spinner("–ò—â–µ–º..."):
            searcher = SemanticSearcher(model_name=model_name, top_k=top_k)
            results = searcher.search(query)
            similarity_results = [
                                    (result["text"], result["score"])
                                    for result in results
                                    if result["score"] >= score_threshold
                                ]
            similarity_results.sort(key=lambda x: x[1], reverse=True)

        st.markdown(f"**–ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å:** `{model_name}`")
        st.subheader("üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")

        if similarity_results:
            max_score = max(score for _, score in similarity_results)

            for i, (doc, score) in enumerate(similarity_results, 1):
                st.markdown(f"**#{i}** ‚Äî score: `{score:.4f}`")
                st.write(highlight(doc[:500], query) + ("..." if len(doc) > 500 else ""))
                st.progress(round(float(score / max_score), 4))
                st.divider()

            # üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è score
            st.subheader("üìä Score-–≥—Ä–∞—Ñ–∏–∫:")
            df = pd.DataFrame(similarity_results, columns=["doc", "score"])
            chart = alt.Chart(df).mark_bar().encode(
                x=alt.X("score:Q", title="Score"),
                y=alt.Y("doc:N", sort="-x", title="–î–æ–∫—É–º–µ–Ω—Ç"),
                tooltip=["score"]
            ).properties(height=300)
            st.altair_chart(chart, use_container_width=True)
    else:
        st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É —Å –∑–∞–¥–∞–Ω–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º.")

# üöÄ –ó–∞–ø—É—Å–∫
if __name__ == "__main__":
    main()
