As machine learning models grow in complexity, interpretability becomes a critical concern. Methods like SHAP and LIME provide local explanations by approximating feature contributions to individual predictions. These techniques are especially valuable in high-stakes domains such as finance and healthcare, where understanding model behavior is essential for trust and accountability.